name: ðŸ¤– Indexer | Scheduled Site Crawl

on:
  # Schedule to run at 00:00 (midnight) and 12:00 (noon) UTC every day (every 12 hours)
  schedule:
    - cron: '0 0,12 * * *' 
  # Allows manual triggering from the GitHub Actions UI for testing
  workflow_dispatch: 
  # Optionally, run on push to main/master branch for immediate testing
  push:
    branches:
      - main
      - master

# Give the workflow permission to write files (i.e., commit the index.json)
permissions:
  contents: write

jobs:
  build_index:
    runs-on: ubuntu-latest
    steps:
      - name: 1. Checkout repository
        uses: actions/checkout@v4
        with:
          # Fetch full history is helpful but not strictly required here. 
          # Setting to 0 ensures actions have the necessary context.
          fetch-depth: 0 

      - name: 2. Set up Python (v3.x)
        # Using the latest recommended version
        uses: actions/setup-python@v6
        with:
          python-version: '3.x'

      - name: 3. Install Python dependencies
        # Dependencies needed for the crawler script
        run: |
          python -m pip install --upgrade pip
          pip install httpx beautifulsoup4

      - name: 4. Run Python Crawler Script
        # Executes crawler.py, which generates index.json
        run: python crawler.py

      - name: 5. Commit and Push new Index.json
        # This action automatically detects and commits new/changed files.
        uses: stefanzweifel/git-auto-commit-action@v6
        with:
          commit_message: 'ðŸ¤– Automated: Update search index.json via GitHub Actions'
          # Targets the current branch
          branch: ${{ github.ref_name }}
          # Only commit the generated index file
          file_pattern: index.json
