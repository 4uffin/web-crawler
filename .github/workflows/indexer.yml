name: ðŸ¤– Indexer | Scheduled Site Crawl

on:

Schedule to run at 00:00 (midnight) and 12:00 (noon) UTC every day (every 12 hours)
schedule:
- cron: '0 0,12 * * *'

Allows manual triggering from the GitHub Actions UI for testing
workflow_dispatch:

Optionally, run on push to main/master branch for immediate testing
push:
branches:
- main
- master

Give the workflow permission to write files (i.e., commit the index.json)
permissions:
contents: write

jobs:
build_index:
runs-on: ubuntu-latest
steps:
- name: 1. Checkout repository
uses: actions/checkout@v4
with:
# Fetch full history is helpful but not strictly required here.
fetch-depth: 0

  - name: 2. Set up Python (v3.x)
    uses: actions/setup-python@v6
    with:
      python-version: '3.x'

  - name: 3. Install Python dependencies
    run: |
      python -m pip install --upgrade pip
      pip install httpx beautifulsoup4

  - name: 4. Run Python Crawler Script
    # Executes crawler.py, which generates index.json
    run: python crawler.py
    env: # <--- CRITICAL UPDATE: Pass the target URL as an environment variable
      CRAWLER_TARGET_URL: 'https://4uffin.github.io/web-crawler/' # <-- REPLACE with your site's root URL!

  - name: 5. Commit and Push new Index.json
    # This action automatically detects and commits new/changed files.
    uses: stefanzweifel/git-auto-commit-action@v6
    with:
      commit_message: 'ðŸ¤– Automated: Update search index.json via GitHub Actions'
      # Targets the current branch
      branch: ${{ github.ref_name }}
      # Only commit the generated index file
      file_pattern: index.json
